{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd7b0fa",
   "metadata": {},
   "source": [
    "### 객체 감지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce36d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import platform\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "weights_path = \"yolo/yolov3.weights\"\n",
    "config_path = \"yolo/yolov3.cfg\"\n",
    "names_path = \"yolo/coco.names\"\n",
    "\n",
    "with open(names_path, \"r\", encoding=\"utf-8\") as coco_file :\n",
    "    label_list = coco_file.read().strip().split('\\n')\n",
    "\n",
    "net = cv2.dnn.readNet(weights_path, config_path)\n",
    "\n",
    "def random_color() :\n",
    "  import random\n",
    "  return (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) \n",
    "\n",
    "def get_font() :\n",
    "  import platform\n",
    "  font_size = 20\n",
    "  try:\n",
    "      if platform.system() == \"Windows\":\n",
    "          # 윈도우: 맑은 고딕\n",
    "          return ImageFont.truetype(\"malgun.ttf\", font_size)\n",
    "      elif platform.system() == \"Darwin\":  # macOS\n",
    "          # 맥: 애플 고딕\n",
    "          return ImageFont.truetype(\"AppleGothic.ttf\", font_size)\n",
    "      else:  # Linux 등\n",
    "          # 기본 폰트 (한글 지원 안 될 수 있음)\n",
    "          return ImageFont.load_default(size=font_size)\n",
    "  except IOError:\n",
    "      # 지정한 폰트 파일이 없을 경우 PIL 기본 폰트 사용\n",
    "      return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def detect_objects(image_array) :\n",
    "\n",
    "    image_array = image_array.copy()\n",
    "\n",
    "    image_height, image_width = image_array.shape[:2]\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(image_array, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "\n",
    "    out_layer_list = net.getUnconnectedOutLayersNames()\n",
    "    detection_list = net.forward(out_layer_list)\n",
    "\n",
    "    bounding_box_list = list()\n",
    "    confidence_list = list()\n",
    "    label_index_list = list()\n",
    "\n",
    "    for prediction_list in detection_list :\n",
    "        color = random_color()\n",
    "        for prediction in prediction_list :\n",
    "            bounding_box = prediction[:4] * np.array([image_width, image_height, image_width, image_height])\n",
    "            center_x, center_y, w, h = bounding_box.astype('int')\n",
    "            label_score_list = prediction[5:]\n",
    "\n",
    "            x = int(center_x - w/2)\n",
    "            y = int(center_y - h/2)\n",
    "\n",
    "            label_index = np.argmax(label_score_list)\n",
    "            confidence = label_score_list[label_index]\n",
    "\n",
    "            if confidence > 0 :\n",
    "                bounding_box_list.append([x, y, w ,h])\n",
    "                confidence_list.append(confidence)\n",
    "                label_index_list.append(label_index)\n",
    "\n",
    "    # print(len(bounding_box_list), len(confidence_list), len(label_index_list))\n",
    "\n",
    "    extracted_index_list = cv2.dnn.NMSBoxes(bounding_box_list, confidence_list, 0.7, 0.3)\n",
    "    extracted_index_list\n",
    "\n",
    "    for extracted_index in extracted_index_list :\n",
    "        color = random_color()\n",
    "        x, y, w, h = bounding_box_list[extracted_index]\n",
    "        confidence = confidence_list[extracted_index]\n",
    "        label_index = label_index_list[extracted_index]\n",
    "        label_text = label_list[label_index]\n",
    "\n",
    "        # cv2.rectangle(image_array, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.rectangle(image_array, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        # cv2.putText(image_array, f\"{label_text} {confidence:.2f}\", (x, y + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "        cv2.putText(image_array, f\"{label_text} {confidence:.2f}\", (x, y + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    return image_array\n",
    "\n",
    "# image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n",
    "# print(image_array)\n",
    "# Image.fromarray(image_array)\n",
    "# Image.open(BytesIO(image_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1141e15f",
   "metadata": {},
   "source": [
    "### dotenv 관련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "811e987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"AZURE_OAI_KEY\")\n",
    "openai_endpoint = os.getenv(\"AZURE_OAI_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a962ca",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be314660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 1. 감지된 물체 목록\\n\\n| 객체 종류       | 감지 확률 | 이미지 내 위치                          |\\n|------------------|-----------|-----------------------------------------|\\n| 테이블           | 0.95      | 중앙 하단 영역                         |\\n| 음료수 (와인 잔) | 0.92      | 테이블 좌측 및 우측 영역               |\\n| 음식 (접시)     | 0.90      | 테이블 중앙 및 우측 영역               |\\n| 장식 (크리스마스 트리) | 0.85 | 배경 좌측 상단                          |\\n\\n---\\n\\n### 2. 각 물체별 상세 설명\\n\\n#### 1. 테이블\\n- **물체의 특징**: 테이블은 넓고 평평한 표면으로, 여러 가지 음식과 음료를 담고 있습니다. 일반적으로 나무 또는 금속 소재로 만들어질 수 있으며, 다양한 색상과 질감을 가질 수 있습니다.\\n- **물체의 예상되는 역할**: 테이블은 식사와 대화를 위한 공간으로, 가족 및 친구들이 모여 식사하며 소통하는 장소입니다. 파티나 모임에서 중심적인 역할을 하며, 식사의 분위기를 조성하는 중요한 요소입니다.\\n\\n#### 2. 음료수 (와인 잔)\\n- **물체의 특징**: 와인 잔은 유리로 만들어져 있으며, 일반적으로 투명하고 우아한 디자인을 가지고 있습니다. 음료수의 색상은 다양할 수 있으나, 이 이미지에서는 붉은색으로 보입니다.\\n- **물체의 예상되는 역할**: 와인 잔은 식사 중 음료를 제공하는 역할을 하며, 식사의 품격을 높이는 요소로 작용합니다. 또한, 축하의 의미를 담아 소통의 장을 마련하는 역할도 합니다.\\n\\n#### 3. 음식 (접시)\\n- **물체의 특징**: 접시는 다양한 음식이 담겨져 있으며, 색상과 형태가 다양합니다. 주로 세라믹이나 유리로 만들어져 깔끔하고 세련된 디자인을 갖추고 있습니다.\\n- **물체의 예상되는 역할**: 음식은 사람들에게 영양을 제공하고, 함께 나누어 먹는 즐거움을 주는 중요한 요소입니다. 이 경우, 다양한 요리가 제공되어 있어, 모임의 화합과 친목을 도모하는 역할을 합니다.\\n\\n#### 4. 장식 (크리스마스 트리)\\n- **물체의 특징**: 크리스마스 트리는 주로 녹색의 인조 나무로 만들어지며, 다양한 장식품과 조명이 장식되어 있습니다. 장식은 화려한 색상으로 꾸며져 있어 시각적으로 즐거움을 줍니다.\\n- **물체의 예상되는 역할**: 크리스마스 트리는 연말 축제 분위기를 조성하는 중요한 요소로, 사람들이 모여 기념일을 축하하고 즐거운 순간을 나누는 데 기여합니다. 또한, 공간의 분위기를 따뜻하고 환상적으로 만들어 주는 역할을 합니다.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import base64\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def request_gpt(image_array) :\n",
    "\n",
    "    image = Image.fromarray(image_array)\n",
    "    byte_image = io.BytesIO()\n",
    "    image.save(byte_image, format='png')\n",
    "    base64_image = base64.b64encode(byte_image.getvalue()).decode('utf-8')\n",
    "\n",
    "    endpoint = openai_endpoint\n",
    "\n",
    "    headers = {\n",
    "        \"Api-Key\" : openai_api_key\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"\n",
    "                        당신은 YOLO 객체 감지 모델의 분석 결과를 전문적으로 해설하는 AI 분석가입니다.\n",
    "                        제공된 이미지와 함께 전달된 객체 감지 결과(감지 확률, 객체 종류, 위치 등)를 기반으로 하여, 각 감지된 물체에 대해 가장 상세하고 정확하게 설명해야 합니다.\n",
    "                        모든 응답은 한국어로 작성해야 합니다.\n",
    "                        \"\"\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"\n",
    "                        첨부된 사진은 YOLO 모델을 통해 분석되었으며, 해당 모델이 감지한 **모든 물체**에 대한 정보를 바탕으로 상세 분석 보고서를 작성해 주세요.\n",
    "\n",
    "                        **[요청 사항]**\n",
    "                        1.  **감지된 물체 목록**을 '객체 종류', '감지 확률', '이미지 내 위치(바운딩 박스 좌표/대략적 위치)'를 요약해 주세요.\n",
    "                        2.  **각 물체별로** 다음 내용을 포함하여 **자세한 설명**을 항목별로 추가해 주세요.\n",
    "                            * 물체의 특징 (색상, 상태, 용도 등)\n",
    "                            * 물체의 예상되는 역할 또는 주변 환경과의 관계\n",
    "                        3.  **감지되지 않은 영역이나 배경에 대한 설명은 일체 포함하지 마세요.**\n",
    "                        \"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                    }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"max_tokens\": 16384,\n",
    "        \"stop\": None,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(endpoint, headers=headers, json=body)\n",
    "\n",
    "    if response.status_code != 200 :\n",
    "        return None\n",
    "    \n",
    "    response_json = response.json()\n",
    "    content = response_json['choices'][0]['message']['content']\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "TEST_IMAGE_URL = \"https://img.freepik.com/free-photo/close-up-people-preparing-christmas-dinner_23-2149144979.jpg\"\n",
    "response = requests.get(TEST_IMAGE_URL)\n",
    "image_data = response.content\n",
    "image_np = np.frombuffer(image_data, np.uint8)\n",
    "image_array = cv2.imdecode(image_np, cv2.IMREAD_COLOR)\n",
    "\n",
    "request_gpt(image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2fbf7f",
   "metadata": {},
   "source": [
    "### Speech TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84e28474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tts_result_20251211_182600.wav'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "tts_api_key = os.getenv(\"SPEECH_STUDIO_API_KEY\")\n",
    "tts_endpoint = os.getenv(\"TTS_KR_ENDPOINT\")\n",
    "\n",
    "def request_tts(text) :\n",
    "\n",
    "    endpoint = tts_endpoint\n",
    "\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\" : tts_api_key,\n",
    "        \"Content-Type\" : \"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\" : \"audio-16khz-128kbitrate-mono-mp3\"\n",
    "    }\n",
    "\n",
    "    cleaned_text = re.sub(r'[^가-힣a-zA-Z0-9\\s%,\\.]', '', text)\n",
    "\n",
    "    body = f\"\"\"\n",
    "        <speak version='1.0' xml:lang='ko-KR'>\n",
    "            <voice xml:lang='ko-KR' xml:gender='Male' name='ko-KR-InJoonNeural'>\n",
    "                {cleaned_text}\n",
    "            </voice>\n",
    "        </speak>\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers, data=body)\n",
    "\n",
    "    if response.status_code != 200 :\n",
    "        return None\n",
    "    \n",
    "    file_name = \"tts_result_{}.wav\".format(datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "    with open(file_name, \"wb\") as audio_file :\n",
    "        audio_file.write(response.content)\n",
    "\n",
    "    return file_name\n",
    "\n",
    "request_tts(\"안녕하세요 만나서 반갑습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd93d76",
   "metadata": {},
   "source": [
    "### Gradio 화면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ea84747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EL91\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\components\\image.py:146: UserWarning: The `mirror_webcam` parameter is deprecated. Please use the `webcam_options` parameter with a `gr.WebcamOptions` instance instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo :\n",
    "\n",
    "    def stream_webcam(image_array) :\n",
    "        result_image_array = detect_objects(image_array)\n",
    "        return result_image_array\n",
    "    \n",
    "    def click_capture(image_array) :\n",
    "        if image_array is None :\n",
    "            return gr.Error(\"감지된 이미지가 없습니다.\", duration=3)\n",
    "        return image_array\n",
    "    \n",
    "    def click_send_gpt(image_array, histories) :\n",
    "\n",
    "        content = request_gpt(image_array)\n",
    "        label_text = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        histories.append({\n",
    "            \"role\" : \"user\", \"content\" : gr.Image(label=label_text, value=image_array)\n",
    "        })\n",
    "        histories.append({\n",
    "            \"role\" : \"assistant\", \"content\" : content\n",
    "        })\n",
    "\n",
    "        return histories\n",
    "\n",
    "    def change_chatbot(histories) :\n",
    "        content = histories[-1]['content']\n",
    "        file_name =  request_tts(content)\n",
    "        return file_name\n",
    "\n",
    "\n",
    "    with gr.Row() :\n",
    "        webcam_image = gr.Image(label=\"실시간 화면\", sources=\"webcam\", mirror_webcam=False)\n",
    "        output_image = gr.Image(label=\"감지 화면\", interactive=False)\n",
    "        capture_image = gr.Image(label=\"이상 징후 발생 화면\", interactive=False)\n",
    "    \n",
    "    with gr.Row() :\n",
    "        capture_button = gr.Button(\"이상 징후 발생\")\n",
    "        send_gpt_button = gr.Button(\"분석\", )\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"분석결과\", type=\"messages\", height=720)\n",
    "    chatbot_audio = gr.Audio(label=\"분석 결과\", interactive=False, autoplay=True)\n",
    "\n",
    "    webcam_image.stream(stream_webcam, inputs=[webcam_image], outputs=[output_image])\n",
    "    capture_button.click(click_capture, inputs=[output_image], outputs=[capture_image])\n",
    "    send_gpt_button.click(click_send_gpt, inputs=[capture_image, chatbot], outputs=[chatbot])\n",
    "    chatbot.change(change_chatbot, inputs=[chatbot], outputs=[chatbot_audio])\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
